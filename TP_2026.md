# Storage Technologies Lesson Plan: On-Premises and Cloud Storage with Ceph

## Course Overview
**Duration:** 2 Days (16 hours total)  
**Level:** Intermediate  
**Prerequisites:** Basic Linux command-line knowledge, understanding of virtualization concepts

## Learning Objectives
By the end of this course, students will be able to:
- Differentiate between on-premises and cloud storage solutions
- Understand the three primary storage types: block, file, and object storage
- Deploy and configure a Ceph storage cluster
- Expose and utilize multiple storage interfaces from a single Ceph cluster
- Evaluate trade-offs between different storage architectures

---

## Day 1: Storage Fundamentals and Ceph Architecture

### Session 1: Storage Fundamentals (2 hours)

#### 1.1 Introduction to Storage Paradigms (45 minutes)
**Topics:**
- Evolution of storage: DAS → SAN → NAS → Cloud
- On-premises vs. cloud storage
  - Capital vs. operational expenditure
  - Control and compliance considerations
  - Performance and latency trade-offs
  - Hybrid storage strategies

**Activities:**
- Group discussion: Real-world scenarios for each storage type
- Whiteboard exercise: Draw your organization's current storage architecture

#### 1.2 Storage Types Deep Dive (75 minutes)
**Topics:**

**Storage Types Comparison Diagram:**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Storage Type Comparison                       │
└─────────────────────────────────────────────────────────────────┘

BLOCK STORAGE (RBD)
┌──────────────────────────────────────────────────────────┐
│  Application → Filesystem → Block Device → Network       │
│                                                           │
│  [Database] → [ext4] → [/dev/rbd0] → [Ceph Cluster]     │
│                                                           │
│  Characteristics:                                         │
│  • Raw block-level access                               │
│  • Requires filesystem on top                           │
│  • Low latency, high performance                        │
│  • One client at a time (unless clustered FS)          │
│  • Perfect for: VMs, databases, high-IOPS workloads     │
└──────────────────────────────────────────────────────────┘

FILE STORAGE (CephFS)
┌──────────────────────────────────────────────────────────┐
│  Application → Filesystem → Network                       │
│                                                           │
│  [App] → [/mnt/cephfs/data.txt] → [Ceph Cluster]        │
│                                                           │
│  Characteristics:                                         │
│  • Hierarchical file/folder structure                    │
│  • POSIX-compliant (standard file operations)            │
│  • Multiple concurrent clients                           │
│  • Shared access with locking                           │
│  • Perfect for: Shared storage, home directories, NAS   │
└──────────────────────────────────────────────────────────┘

OBJECT STORAGE (S3/RGW)
┌──────────────────────────────────────────────────────────┐
│  Application → HTTP/S3 API → Network                      │
│                                                           │
│  [App] → PUT /bucket/object → [Ceph Cluster]             │
│                                                           │
│  Characteristics:                                         │
│  • Flat namespace (buckets + objects)                    │
│  • Rich metadata support                                 │
│  • RESTful API (HTTP/HTTPS)                             │
│  • Massive scalability                                   │
│  • Perfect for: Backups, archives, media, cloud apps    │
└──────────────────────────────────────────────────────────┘

                    Side-by-Side Comparison
┌──────────┬────────────┬────────────┬─────────────────┐
│ Feature  │   Block    │    File    │     Object      │
├──────────┼────────────┼────────────┼─────────────────┤
│ Access   │ Block-level│ File-level │ HTTP/REST API   │
│ Protocol │ RBD/iSCSI  │ NFS/CIFS   │ S3/Swift        │
│ Structure│ Raw blocks │ Directories│ Flat/Buckets    │
│ Sharing  │ Single*    │ Multiple   │ Multiple        │
│ Metadata │ Minimal    │ Standard   │ Extensive       │
│ Use Case │ Databases  │ File shares│ Archives/Backup │
│ Scale    │ Moderate   │ Good       │ Massive         │
│ Latency  │ Lowest     │ Low        │ Higher          │
│ Cost/TB  │ High       │ Medium     │ Low             │
└──────────┴────────────┴────────────┴─────────────────┘
* Unless using clustered filesystem

Real-World Example Architecture:
┌──────────────────────────────────────────────────────────┐
│                     Organization                          │
│                                                           │
│  ┌─────────────┐      ┌──────────────┐                  │
│  │ Production  │      │   Backup     │                  │
│  │  Database   │      │   System     │                  │
│  │    (RBD)    │      │    (S3)      │                  │
│  └──────┬──────┘      └──────┬───────┘                  │
│         │                    │                           │
│         │         ┌──────────┴────────┐                 │
│         │         │   Home Directories│                 │
│         │         │     (CephFS)      │                 │
│         │         └──────────┬────────┘                 │
│         │                    │                           │
│         └────────────────────┼──────────────────────┐   │
│                              │                       │   │
│                    ┌─────────▼────────┐             │   │
│                    │   Ceph Cluster   │◄────────────┘   │
│                    │   (Unified)      │                 │
│                    └──────────────────┘                 │
│                                                          │
│  One cluster, three interfaces, different use cases     │
└──────────────────────────────────────────────────────────┘
```

**Block Storage:**
- Direct block-level access
- Use cases: Databases, virtual machine disks, high-performance applications
- Protocols: iSCSI, Fibre Channel, RBD
- Advantages: Performance, flexibility
- Disadvantages: Complexity, requires filesystem management

**File Storage:**
- Hierarchical file system access
- Use cases: Shared documents, home directories, application data
- Protocols: NFS, SMB/CIFS, CephFS
- Advantages: Familiar interface, easy sharing
- Disadvantages: Metadata bottlenecks, scaling challenges

**Object Storage:**
- Flat namespace with metadata
- Use cases: Backups, archives, multimedia, cloud-native applications
- Protocols: S3, Swift
- Advantages: Massive scalability, metadata richness, cost-effective
- Disadvantages: No filesystem semantics, eventual consistency models

**Activities:**
- Case study analysis: Match storage types to business requirements
- Quiz: Identify appropriate storage type for given scenarios

---

### Session 2: Introduction to Ceph (2 hours)

#### 2.1 What is Ceph? (30 minutes)
**Topics:**
- History and development of Ceph
- Open-source distributed storage system
- Unified storage platform (block + file + object)
- Key features:
  - Self-healing
  - Self-managing
  - No single point of failure
  - Scalability to exabyte level

#### 2.2 Ceph Architecture (60 minutes)
**Topics:**

**Ceph Architecture Overview:**
```
                    ┌─────────────────────────────────────┐
                    │      Client Applications            │
                    │  (VMs, Containers, Bare Metal)      │
                    └──────────┬──────────────────────────┘
                               │
        ┌──────────────────────┼──────────────────────┐
        │                      │                      │
        ▼                      ▼                      ▼
┌───────────────┐    ┌─────────────────┐    ┌────────────────┐
│  RADOS Block  │    │    CephFS       │    │  RADOS Gateway │
│  Device (RBD) │    │ (File Storage)  │    │ (Object Store) │
│               │    │                 │    │   S3 / Swift   │
└───────┬───────┘    └────────┬────────┘    └────────┬───────┘
        │                     │                       │
        │              ┌──────┴──────┐               │
        │              │     MDS     │               │
        │              │  (Metadata) │               │
        │              └─────────────┘               │
        │                                            │
        └────────────────────┬───────────────────────┘
                             │
                    ┌────────▼────────┐
                    │   librados      │
                    │ (Storage Layer) │
                    └────────┬────────┘
                             │
            ┌────────────────┼────────────────┐
            │                │                │
            ▼                ▼                ▼
      ┌─────────┐      ┌─────────┐     ┌─────────┐
      │   MON   │      │   MGR   │     │   OSD   │
      │ Monitor │◄────►│ Manager │     │ Daemon  │
      │ Cluster │      │Dashboard│     │ Storage │
      │  State  │      │Metrics  │     │         │
      └─────────┘      └─────────┘     └────┬────┘
                                             │
                                    ┌────────┴────────┐
                                    │  Physical Disks │
                                    │   (Data Store)  │
                                    └─────────────────┘

Data Flow:
1. Client → librados → CRUSH algorithm → OSD
2. No central metadata lookup (scalable)
3. Data distributed via CRUSH map
4. OSDs handle replication & recovery autonomously
```

**Core Components:**
- **MON (Monitor):** Maintains cluster state, consensus through Paxos
- **OSD (Object Storage Daemon):** Stores data, handles replication, recovery
- **MGR (Manager):** Cluster monitoring, management, dashboard
- **MDS (Metadata Server):** Required for CephFS file storage

**Data Distribution:**
- CRUSH algorithm (Controlled Replication Under Scalable Hashing)
- Placement groups (PGs)
- Pools and replication strategies

**Storage Interfaces:**
- **RADOS:** Foundation object store
- **RBD (RADOS Block Device):** Block storage
- **CephFS:** POSIX-compliant filesystem
- **RGW (RADOS Gateway):** S3/Swift compatible object storage

**Activities:**
- Diagram exercise: Draw Ceph architecture components
- Discussion: How CRUSH algorithm eliminates bottlenecks

#### 2.3 Lab Environment Setup (30 minutes)
**Topics:**
- Lab architecture overview
- Virtual machine specifications
- Network configuration requirements

**Lab Setup Specifications:**
- 1 Admin node (for deployment)
- 3 OSD nodes (each with 2-3 virtual disks)
- 3 MON nodes (can be co-located with OSD nodes)
- 1 MGR node (can be co-located)
- Network: 10.0.0.0/24 (cluster network), 192.168.1.0/24 (public network)

**Lab Architecture Diagram:**
```
┌─────────────────────────────────────────────────────────────────┐
│                      Hypervisor (VirtualBox/KVM/VMware)         │
│                                                                 │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │   Node 1     │  │   Node 2     │  │   Node 3     │         │
│  │              │  │              │  │              │         │
│  │ - MON        │  │ - MON        │  │ - MON        │         │
│  │ - MGR        │  │ - MGR        │  │              │         │
│  │ - OSD.0      │  │ - OSD.1      │  │ - OSD.2      │         │
│  │ - OSD.3      │  │ - OSD.4      │  │ - OSD.5      │         │
│  │ - Admin      │  │              │  │              │         │
│  │              │  │              │  │              │         │
│  │ eth0: Public │  │ eth0: Public │  │ eth0: Public │         │
│  │ 192.168.1.11 │  │ 192.168.1.12 │  │ 192.168.1.13 │         │
│  │              │  │              │  │              │         │
│  │ eth1: Cluster│  │ eth1: Cluster│  │ eth1: Cluster│         │
│  │ 10.0.0.11    │  │ 10.0.0.12    │  │ 10.0.0.13    │         │
│  │              │  │              │  │              │         │
│  │ Disks:       │  │ Disks:       │  │ Disks:       │         │
│  │ - sda (OS)   │  │ - sda (OS)   │  │ - sda (OS)   │         │
│  │ - sdb (OSD)  │  │ - sdb (OSD)  │  │ - sdb (OSD)  │         │
│  │ - sdc (OSD)  │  │ - sdc (OSD)  │  │ - sdc (OSD)  │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
│         │                 │                 │                  │
│         └─────────────────┴─────────────────┘                  │
│                           │                                    │
│                  ┌────────┴────────┐                          │
│                  │  Virtual Switch  │                          │
│                  │  (Public Network)│                          │
│                  └────────┬────────┘                          │
│                           │                                    │
│                  ┌────────┴────────┐                          │
│                  │  Virtual Switch  │                          │
│                  │(Cluster Network) │                          │
│                  └─────────────────┘                          │
└─────────────────────────────────────────────────────────────────┘

Resource Specifications per Node:
- CPU: 2-4 vCPUs
- RAM: 4-8 GB
- OS Disk (sda): 20 GB
- OSD Disks (sdb, sdc): 10-20 GB each
- Network: 2 NICs (NAT/Bridged for public, Host-only for cluster)
```

**Activities:**
- Provision VMs using VirtualBox/KVM/VMware
- Verify network connectivity between nodes
- Configure hostnames and /etc/hosts

---

### Session 3: Ceph Cluster Deployment (2 hours)

#### 3.1 Installation Methods (20 minutes)
**Topics:**
- cephadm (recommended method)
- ceph-deploy (legacy)
- Manual deployment
- Choosing the right method for your environment

#### 3.2 Deploying Ceph with cephadm (100 minutes)
**Step-by-step deployment:**

**Deployment Architecture Flow:**
```
Step 1: Bootstrap
┌─────────────────────────────────────────────────────┐
│                    Node 1 (Admin)                    │
│                                                      │
│  $ sudo cephadm bootstrap --mon-ip 192.168.1.11    │
│                                                      │
│  Creates:                                           │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐         │
│  │   MON    │  │   MGR    │  │   OSD    │         │
│  │(Monitor) │  │(Manager) │  │ (if disk)│         │
│  └──────────┘  └──────────┘  └──────────┘         │
│                                                      │
│  Generates: Admin keyring, config, dashboard        │
└─────────────────────────────────────────────────────┘

Step 2: Add Hosts
┌─────────────────────────────────────────────────────┐
│  Node 1 (Admin)         Node 2            Node 3    │
│  ┌─────────┐          ┌─────────┐      ┌─────────┐ │
│  │   MON   │          │ (empty) │      │ (empty) │ │
│  │   MGR   │  SSH────►│         │      │         │ │
│  └─────────┘   Keys   └─────────┘      └─────────┘ │
│                                                      │
│  $ sudo ceph orch host add node2                   │
│  $ sudo ceph orch host add node3                   │
└─────────────────────────────────────────────────────┘

Step 3: Deploy Services
┌─────────────────────────────────────────────────────┐
│   Node 1            Node 2            Node 3        │
│  ┌─────────┐      ┌─────────┐      ┌─────────┐    │
│  │   MON   │      │   MON   │      │   MON   │    │
│  │   MGR   │      │   MGR   │      │         │    │
│  │  OSD.0  │      │  OSD.1  │      │  OSD.2  │    │
│  │  OSD.3  │      │  OSD.4  │      │  OSD.5  │    │
│  └─────────┘      └─────────┘      └─────────┘    │
│       │                │                │           │
│       └────────────────┴────────────────┘           │
│                        │                            │
│              ┌─────────▼─────────┐                 │
│              │  Cluster Network  │                 │
│              │   (Heartbeats,    │                 │
│              │   Replication)    │                 │
│              └───────────────────┘                 │
└─────────────────────────────────────────────────────┘

Final Cluster State:
┌──────────────────────────────────────────────────────┐
│              Ceph Cluster Status                     │
│                                                      │
│  Health: HEALTH_OK                                   │
│  Monitors: 3 (node1, node2, node3)                  │
│  Managers: 2 (node1 active, node2 standby)          │
│  OSDs: 6 total, 6 up, 6 in                          │
│  Pools: 1 (device_health_metrics)                   │
│  Data: 0 B used, 120 GB available                   │
│                                                      │
│  Services Running:                                   │
│  • Dashboard: https://node1:8443                    │
│  • Prometheus: http://node1:9095                    │
│  • Grafana: http://node1:3000                       │
└──────────────────────────────────────────────────────┘
```

**Preparation (20 minutes):**
```bash
# On all nodes
sudo apt-get update
sudo apt-get install -y docker.io python3 chrony
sudo systemctl enable --now docker
sudo systemctl enable --now chrony

# Configure firewall rules
sudo ufw allow 22/tcp
sudo ufw allow 3300/tcp
sudo ufw allow 6789/tcp
sudo ufw allow 6800:7300/tcp
```

**Bootstrap the Cluster (20 minutes):**
```bash
# On admin node
sudo apt install -y cephadm
sudo cephadm add-repo --release reef
sudo cephadm install

# Install ceph command line tools
sudo cephadm install ceph-common

# Bootstrap first monitor
sudo cephadm bootstrap --mon-ip <admin-node-ip>

# Save the admin credentials shown
```

**Add Hosts (20 minutes):**
```bash
# Copy SSH keys to all nodes
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2 
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3

# Add hosts to cluster
sudo ceph orch host add node2
sudo ceph orch host add node3

# Verify hosts
sudo ceph orch host ls
```

**Deploy Services (40 minutes):**
```bash
# Deploy additional monitors
sudo ceph orch apply mon --placement="3 node1 node2 node3"

# Deploy managers
sudo ceph orch apply mgr --placement="2 node1 node2"

# Add OSDs (automatically discovers available disks)
sudo ceph orch apply osd --all-available-devices

# Or add OSDs manually
sudo ceph orch daemon add osd node1:/dev/sdb
sudo ceph orch daemon add osd node2:/dev/sdb

# Verify cluster health
sudo ceph -s
sudo ceph osd tree
```

**Activities:**
- Deploy Ceph cluster following the guided lab
- Troubleshoot any deployment issues
- Verify all services are running

---

### Session 4: Ceph Dashboard and Monitoring (2 hours)

#### 4.1 Accessing the Dashboard (30 minutes)
**Topics:**
- Dashboard features overview
- Accessing via HTTPS
- User management and authentication

**Activities:**
```bash
# Get dashboard URL and credentials
sudo ceph mgr services

# Create additional dashboard user
sudo ceph dashboard ac-user-create <username> <password> administrator

# Access dashboard at https://<admin-node-ip>:8443
```

#### 4.2 Cluster Health Monitoring (45 minutes)
**Topics:**
- Understanding HEALTH_OK, HEALTH_WARN, HEALTH_ERR
- Common warning messages and resolutions
- PG states and what they mean
- Cluster capacity planning

**Key Commands:**
```bash
sudo ceph health detail
sudo ceph osd status
sudo ceph df
sudo ceph osd df
sudo ceph pg stat
```

**Activities:**
- Navigate through dashboard sections
- Interpret cluster health messages
- Monitor OSD performance metrics

#### 4.3 Pool Management (45 minutes)
**Topics:**
- Creating and configuring pools
- Replication vs. erasure coding
- PG calculation and optimization
- Pool quotas and limits

**Activities:**
```bash
# Create replicated pool
sudo ceph osd pool create mypool 32 32

# Set pool replication size
sudo ceph osd pool set mypool size 3
sudo ceph osd pool set mypool min_size 2

# Create erasure-coded pool
sudo ceph osd erasure-code-profile set myprofile k=2 m=1
sudo ceph osd pool create ec-pool 32 32 erasure myprofile

# List pools
sudo ceph osd lspools
sudo ceph osd pool ls detail
```

**End of Day 1 Review (15 minutes):**
- Q&A session
- Recap key concepts
- Preview Day 2 activities

---

## Day 2: Storage Interfaces and Use Cases

### Session 5: Block Storage with RBD (2 hours)

#### 5.1 RADOS Block Device Concepts (30 minutes)
**Topics:**
- RBD architecture and features
- Thin provisioning
- Snapshots and clones
- RBD mirroring for disaster recovery
- Integration with virtualization platforms

**RBD Architecture Diagram:**
```
┌──────────────────────────────────────────────────────────────┐
│                    Client / VM Host                          │
│                                                              │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐           │
│  │   VM #1    │  │   VM #2    │  │   VM #3    │           │
│  │            │  │            │  │            │           │
│  │  /dev/vda  │  │  /dev/vda  │  │  /dev/vda  │           │
│  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘           │
│        │               │               │                   │
│        └───────────────┴───────────────┘                   │
│                        │                                    │
│              ┌─────────▼─────────┐                         │
│              │   librbd / krbd   │                         │
│              │  (RBD Client)     │                         │
│              └─────────┬─────────┘                         │
└────────────────────────┼──────────────────────────────────┘
                         │
                         │ Network (iSCSI-like protocol)
                         │
┌────────────────────────▼──────────────────────────────────┐
│                    Ceph Cluster                            │
│                                                            │
│  ┌──────────────────────────────────────┐                │
│  │         RBD Pool (rbd)               │                │
│  │                                      │                │
│  │  ┌─────────┐  ┌─────────┐  ┌──────────────┐         │
│  │  │ vm1-disk│  │ vm2-disk│  │ vm3-disk     │         │
│  │  │ (10 GB) │  │ (20 GB) │  │ (15 GB)      │         │
│  │  └────┬────┘  └────┬────┘  └──────┬───────┘         │
│  │       │            │              │                  │
│  │       └────────────┴──────────────┘                  │
│  │                    │                                  │
│  │         ┌──────────▼──────────┐                      │
│  │         │ RADOS Objects        │                      │
│  │         │ (4MB chunks)         │                      │
│  │         └──────────┬───────────┘                      │
│  └────────────────────┼──────────────────────────────────┘
│                       │                                   │
│       ┌───────────────┼───────────────┐                  │
│       ▼               ▼               ▼                  │
│   ┌───────┐      ┌───────┐      ┌───────┐              │
│   │ OSD.0 │      │ OSD.1 │      │ OSD.2 │              │
│   │/dev/  │      │/dev/  │      │/dev/  │              │
│   │sdb    │      │sdb    │      │sdb    │              │
│   └───────┘      └───────┘      └───────┘              │
│                                                          │
│   Each RBD image is striped across multiple OSDs        │
│   Default stripe size: 4MB                              │
│   Replication factor: 3 (configurable)                  │
└──────────────────────────────────────────────────────────┘

RBD Features:
┌─────────────────────────────────────────────────────────┐
│ Snapshots:  image@snapshot1 ──┐                        │
│                                ├──► Protected snapshot  │
│ Clone:      image-clone  ◄─────┘    (read-only)        │
│                                                          │
│ Thin Provisioning: Allocate space on write              │
│ Fast Clone: Copy-on-write from snapshot                 │
│ Layering: Base image + differential clones              │
└─────────────────────────────────────────────────────────┘
```

#### 5.2 Creating and Using RBD Images (90 minutes)
**Setup RBD Pool:**
```bash
# Create pool for RBD
sudo ceph osd pool create rbd 32 32
sudo ceph osd pool application enable rbd rbd

# Initialize RBD
sudo rbd pool init rbd
```

**Create and Map Block Devices:**
```bash
# Create RBD image
sudo rbd create --size 10G rbd/mydisk

# List images
sudo rbd ls rbd
sudo rbd info rbd/mydisk

# Map to local device
sudo rbd map rbd/mydisk

# Check mapped devices
sudo rbd showmapped

# Create filesystem
sudo mkfs.ext4 /dev/rbd0

# Mount and use
sudo mkdir /mnt/ceph-block
sudo mount /dev/rbd0 /mnt/ceph-block

# Test write performance
sudo dd if=/dev/zero of=/mnt/ceph-block/testfile bs=1M count=1024
```

**Snapshots and Clones:**
```bash
# Create snapshot
sudo rbd snap create rbd/mydisk@snapshot1

# List snapshots
sudo rbd snap ls rbd/mydisk

# Protect snapshot (required for cloning)
sudo rbd snap protect rbd/mydisk@snapshot1

# Clone snapshot
sudo rbd clone rbd/mydisk@snapshot1 rbd/mydisk-clone

# Flatten clone (remove dependency)
sudo rbd flatten rbd/mydisk-clone
```

**Activities:**
- Create multiple RBD images
- Test read/write performance
- Create snapshot and restore from it
- Experiment with clones

---

### Session 6: Object Storage with RADOS Gateway (2 hours)

#### 6.1 RADOS Gateway Architecture (20 minutes)
**Topics:**
- RGW as S3/Swift API gateway
- Multi-tenancy support
- Integration with authentication systems
- Use cases: Backups, static websites, cloud-native apps

**RADOS Gateway Architecture Diagram:**
```
┌────────────────────────────────────────────────────────────┐
│                     Client Applications                     │
│                                                            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌──────────┐    │
│  │ s3cmd   │  │ boto3   │  │ AWS CLI │  │  Browser │    │
│  │ (CLI)   │  │ (Python)│  │         │  │  (HTTP)  │    │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬─────┘    │
│       │            │            │            │            │
│       └────────────┴────────────┴────────────┘            │
│                    │                                       │
│                    │ S3 / Swift API (HTTP/HTTPS)          │
└────────────────────┼────────────────────────────────────┘
                     │
┌────────────────────▼────────────────────────────────────┐
│                RADOS Gateway (RGW)                       │
│               Running on Node 1:8080                     │
│                                                          │
│  ┌────────────────────────────────────────────┐        │
│  │         RGW Service Components             │        │
│  │                                            │        │
│  │  ┌──────────────┐    ┌──────────────┐    │        │
│  │  │  S3 API      │    │ Swift API    │    │        │
│  │  │  Handler     │    │ Handler      │    │        │
│  │  └──────┬───────┘    └──────┬───────┘    │        │
│  │         │                   │             │        │
│  │         └───────────┬───────┘             │        │
│  │                     │                      │        │
│  │         ┌───────────▼────────────┐        │        │
│  │         │   Authentication       │        │        │
│  │         │   (Access/Secret Keys) │        │        │
│  │         └───────────┬────────────┘        │        │
│  │                     │                      │        │
│  │         ┌───────────▼────────────┐        │        │
│  │         │   Request Processing   │        │        │
│  │         │   - Bucket operations  │        │        │
│  │         │   - Object operations  │        │        │
│  │         │   - Multipart upload   │        │        │
│  │         └───────────┬────────────┘        │        │
│  └─────────────────────┼──────────────────────┘        │
│                        │                                │
│            ┌───────────▼───────────┐                   │
│            │      librados         │                   │
│            └───────────┬───────────┘                   │
└────────────────────────┼───────────────────────────────┘
                         │
┌────────────────────────▼───────────────────────────────┐
│                   Ceph Cluster                          │
│                                                         │
│  ┌─────────────────────────────────────────┐          │
│  │   RGW Data Pools                        │          │
│  │                                         │          │
│  │  .rgw.root       ─ Root pool           │          │
│  │  .rgw.control    ─ Control info        │          │
│  │  .rgw.meta       ─ Metadata pool       │          │
│  │  .rgw.log        ─ Log pool            │          │
│  │  .rgw.buckets    ─ Bucket index        │          │
│  │  .rgw.buckets.data ─ Object data       │          │
│  │                                         │          │
│  │  ┌──────────────────────────────────┐  │          │
│  │  │  Bucket: mybucket                │  │          │
│  │  │    - file1.txt (object)          │  │          │
│  │  │    - file2.jpg (object)          │  │          │
│  │  │    - data/report.pdf (object)    │  │          │
│  │  └──────────┬───────────────────────┘  │          │
│  └─────────────┼──────────────────────────┘          │
│                │                                       │
│    ┌───────────┼───────────────┐                     │
│    ▼           ▼               ▼                     │
│ ┌──────┐   ┌──────┐        ┌──────┐                 │
│ │OSD.0 │   │OSD.1 │        │OSD.2 │                 │
│ └──────┘   └──────┘        └──────┘                 │
│                                                       │
│ Objects stored with replication factor 3             │
└───────────────────────────────────────────────────────┘

S3 API Request Flow:
┌──────────────────────────────────────────────────────┐
│ 1. Client → PUT /mybucket/file.txt                  │
│ 2. RGW authenticates using access/secret key        │
│ 3. RGW creates object in .rgw.buckets.data pool     │
│ 4. Object split into 4MB chunks (RADOS objects)     │
│ 5. CRUSH distributes chunks across OSDs             │
│ 6. RGW returns success to client                    │
└──────────────────────────────────────────────────────┘
```

#### 6.2 Deploying RADOS Gateway (40 minutes)
**Deploy RGW:**
```bash
# Deploy RGW service
sudo ceph orch apply rgw myrgw --placement="1 node1" --port=8080

# Verify RGW is running
sudo ceph orch ls
sudo ceph -s
```

**Create S3 User:**
```bash
# Create S3 user
sudo radosgw-admin user create --uid=testuser --display-name="Test User"

# Note the access_key and secret_key from output

# List users
sudo radosgw-admin user list
sudo radosgw-admin user info --uid=testuser
```

#### 6.3 Using Object Storage (60 minutes)
**Install and Configure S3 Client:**
```bash
# Install s3cmd
sudo apt-get install s3cmd

# Configure s3cmd
s3cmd --configure
# Enter access_key and secret_key
# Host: <node1-ip>:8080
# Use HTTPS: No
```

**S3 Operations:**
```bash
# Create bucket
s3cmd mb s3://mybucket

# List buckets
s3cmd ls

# Upload file
echo "Hello Ceph Object Storage" > testfile.txt
s3cmd put testfile.txt s3://mybucket/

# List objects
s3cmd ls s3://mybucket/

# Download file
s3cmd get s3://mybucket/testfile.txt downloaded.txt

# Set bucket permissions (public read)
s3cmd setacl s3://mybucket --acl-public

# Delete object
s3cmd del s3://mybucket/testfile.txt
```

**Python Boto3 Example:**
```python
import boto3

# Configure client
s3 = boto3.client('s3',
    endpoint_url='http://<node1-ip>:8080',
    aws_access_key_id='<access_key>',
    aws_secret_access_key='<secret_key>'
)

# Create bucket
s3.create_bucket(Bucket='python-bucket')

# Upload file
with open('data.txt', 'rb') as f:
    s3.upload_fileobj(f, 'python-bucket', 'data.txt')

# List objects
response = s3.list_objects_v2(Bucket='python-bucket')
for obj in response.get('Contents', []):
    print(obj['Key'])

# Download file
s3.download_file('python-bucket', 'data.txt', 'downloaded.txt')
```

**Activities:**
- Create S3 users with different permissions
- Upload and download files using s3cmd
- Write a Python script to automate bucket operations
- Test bucket versioning and lifecycle policies

---

### Session 7: File Storage with CephFS (2 hours)

#### 7.1 CephFS Architecture (20 minutes)
**Topics:**
- POSIX-compliant distributed filesystem
- Metadata Server (MDS) role and scaling
- Multiple active MDS for performance
- CephFS snapshots and quotas

**CephFS Architecture Diagram:**
```
┌──────────────────────────────────────────────────────────┐
│                    Client Machines                        │
│                                                          │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐             │
│  │ Client 1 │  │ Client 2 │  │ Client 3 │             │
│  │          │  │          │  │          │             │
│  │ /mnt/    │  │ /mnt/    │  │ /mnt/    │             │
│  │ cephfs/  │  │ cephfs/  │  │ cephfs/  │             │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘             │
│       │             │             │                     │
│       │  ┌──────────┴─────────────┘                    │
│       │  │  Kernel CephFS Driver or ceph-fuse          │
│       └──┤  (POSIX Interface)                          │
│          │                                              │
└──────────┼──────────────────────────────────────────────┘
           │
           │ File Operations (open, read, write, stat)
           │
┌──────────▼──────────────────────────────────────────────┐
│              Ceph Cluster - MDS Layer                    │
│                                                          │
│  ┌────────────────────────────────────────┐            │
│  │    Metadata Servers (MDS)              │            │
│  │                                        │            │
│  │  ┌─────────┐         ┌─────────┐     │            │
│  │  │ MDS-0   │         │ MDS-1   │     │            │
│  │  │ Active  │◄───────►│ Standby │     │            │
│  │  │         │ Failover│         │     │            │
│  │  └────┬────┘         └─────────┘     │            │
│  │       │                                │            │
│  │       │ Manages:                       │            │
│  │       │ - Directory hierarchy          │            │
│  │       │ - File metadata (permissions)  │            │
│  │       │ - Inode information            │            │
│  │       │ - Capabilities (distributed)   │            │
│  │       │   lock management              │            │
│  └───────┼────────────────────────────────┘            │
│          │                                              │
│          │ Metadata I/O                                 │
│          ▼                                              │
│  ┌──────────────────┐        ┌──────────────────┐     │
│  │ Metadata Pool    │        │  Data Pool       │     │
│  │ (cephfs_metadata)│        │ (cephfs_data)    │     │
│  │                  │        │                  │     │
│  │ - Directory      │        │ - File contents  │     │
│  │   structures     │        │ - User data      │     │
│  │ - Inodes         │        │                  │     │
│  │ - Capabilities   │        │                  │     │
│  └─────────┬────────┘        └────────┬─────────┘     │
│            │                          │                │
│            └──────────┬───────────────┘                │
│                       │                                │
│                       ▼                                │
│             ┌─────────────────┐                        │
│             │    librados     │                        │
│             └────────┬────────┘                        │
│                      │                                 │
│          ┌───────────┼───────────┐                    │
│          ▼           ▼           ▼                    │
│      ┌──────┐    ┌──────┐    ┌──────┐                │
│      │OSD.0 │    │OSD.1 │    │OSD.2 │                │
│      │      │    │      │    │      │                │
│      └──────┘    └──────┘    └──────┘                │
└──────────────────────────────────────────────────────┘

CephFS File Operation Flow:
┌─────────────────────────────────────────────────────┐
│                                                     │
│  Client wants to read /home/user/document.txt      │
│                                                     │
│  1. Client → MDS: "lookup /home/user/document.txt" │
│     MDS returns: inode info + capabilities         │
│                                                     │
│  2. Client → OSDs: Direct data read                │
│     (Using inode info, bypassing MDS)              │
│                                                     │
│  3. Client caches capability for future access     │
│                                                     │
│  For writes:                                        │
│  - Client requests write capability from MDS       │
│  - MDS grants capability (distributed lock)        │
│  - Client writes directly to OSDs                  │
│  - Client releases capability when done            │
│                                                     │
└─────────────────────────────────────────────────────┘

Directory Structure Example:
/mnt/cephfs/
├── home/
│   ├── alice/
│   │   └── documents/
│   └── bob/
│       └── photos/
├── shared/
│   ├── .snap/              ← Snapshot directory
│   │   ├── snapshot1/      ← Point-in-time copy
│   │   └── snapshot2/
│   └── projects/
└── archive/

Key Features:
┌──────────────────────────────────────────────────┐
│ - POSIX Compliance: Standard filesystem API      │
│ - Snapshots: .snap directories (per directory)   │
│ - Quotas: Per-directory size/file count limits   │
│ - Multiple Active MDS: Horizontal scaling        │
│ - Dynamic Subtree Partitioning: Load balancing   │
└──────────────────────────────────────────────────┘
```

#### 7.2 Deploying CephFS (40 minutes)
**Create CephFS:**
```bash
# Create pools for CephFS
sudo ceph osd pool create cephfs_data 32 32
sudo ceph osd pool create cephfs_metadata 32 32

# Create filesystem
sudo ceph fs new mycephfs cephfs_metadata cephfs_data

# Deploy MDS
sudo ceph orch apply mds mycephfs --placement="2 node1 node2"

# Verify MDS status
sudo ceph fs status mycephfs
sudo ceph mds stat
```

#### 7.3 Mounting and Using CephFS (60 minutes)
**Mount with Kernel Driver:**
```bash
# Install ceph-common on client
sudo apt-get install ceph-common

# Copy ceph.conf and keyring from admin node
sudo scp root@node1:/etc/ceph/ceph.conf /etc/ceph/
sudo scp root@node1:/etc/ceph/ceph.client.admin.keyring /etc/ceph/

# Create mount point
sudo mkdir /mnt/cephfs

# Mount CephFS
sudo mount -t ceph node1:6789:/ /mnt/cephfs -o name=admin,secret=$(sudo grep key /etc/ceph/ceph.client.admin.keyring | awk '{print $3}')

# Or use ceph-fuse (userspace mount)
sudo ceph-fuse /mnt/cephfs

# Verify mount
df -h /mnt/cephfs
```

**CephFS Features:**
```bash
# Create directories and files
sudo mkdir /mnt/cephfs/shared
sudo cp /etc/hosts /mnt/cephfs/shared/

# Set quotas
sudo setfattr -n ceph.quota.max_bytes -v 10737418240 /mnt/cephfs/shared  # 10GB
sudo setfattr -n ceph.quota.max_files -v 10000 /mnt/cephfs/shared

# Get quota information
sudo getfattr -n ceph.quota.max_bytes /mnt/cephfs/shared

# Create snapshot
sudo mkdir /mnt/cephfs/shared/.snap/snapshot1

# List snapshots
ls /mnt/cephfs/shared/.snap/

# Access snapshot data
ls /mnt/cephfs/shared/.snap/snapshot1/

# Remove snapshot
sudo rmdir /mnt/cephfs/shared/.snap/snapshot1
```

**Activities:**
- Mount CephFS on multiple clients
- Test concurrent file access
- Implement directory quotas
- Create and restore from snapshots
- Measure filesystem performance with fio

---

### Session 8: Integration, Best Practices, and Wrap-up (2 hours)

#### 8.1 Real-World Integration Scenarios (45 minutes)
**Topics:**

**Kubernetes Integration:**
- Using Rook operator
- Dynamic volume provisioning
- CSI drivers for RBD and CephFS

**OpenStack Integration:**
- Cinder (block storage) backend
- Glance (image storage) backend
- Nova ephemeral storage

**Backup and Archive:**
- RGW as backup target for Veeam, Bacula
- S3-compatible backup solutions
- Archive tier configuration

**Activities:**
- Discussion: Design a storage architecture for given scenarios
- Group exercise: Plan Ceph deployment for a mid-size organization

#### 8.2 Performance Tuning and Best Practices (45 minutes)
**Topics:**

**Hardware Recommendations:**
- CPU: 1 core per OSD
- RAM: 5GB per OSD minimum
- Network: 10GbE minimum for production
- Disks: SSDs for journals/metadata, HDDs for capacity

**Performance Optimization:**
- PG count tuning
- BlueStore vs. FileStore
- SSD caching with bcache or dm-cache
- Network tuning (jumbo frames, TCP tuning)

**Operational Best Practices:**
- Regular health checks
- Capacity planning (80% rule)
- Rolling upgrades
- Backup strategies
- Documentation and runbooks

**Common Issues and Troubleshooting:**
- Slow requests
- Clock skew problems
- Full OSDs
- PG imbalance

#### 8.3 Course Review and Q&A (30 minutes)
**Activities:**
- Review learning objectives
- Open Q&A session
- Student presentations: Share lab findings
- Course evaluation
- Next steps and further learning resources

---

## Lab Deliverables

Students should complete and document the following:

1. **Architecture Diagram**
   - Complete Ceph cluster topology
   - Network layout
   - Service placement

2. **Configuration Documentation**
   - Cluster deployment steps
   - Pool configurations
   - Service configurations

3. **Storage Interface Demonstrations**
   - RBD: Create, mount, and benchmark block device
   - RGW: Upload/download objects via S3 API
   - CephFS: Mount and demonstrate file operations

4. **Performance Testing Results**
   - Benchmark results for each storage type
   - Analysis of performance characteristics

5. **Troubleshooting Log**
   - Issues encountered during lab
   - Resolution steps taken

---

## Assessment Criteria

**Practical Skills (60%)**
- Successful Ceph cluster deployment
- Configuration of all three storage interfaces
- Demonstration of key features (snapshots, clones, etc.)

**Understanding (30%)**
- Ability to explain architecture decisions
- Understanding of use cases for each storage type
- Troubleshooting approach

**Documentation (10%)**
- Quality of lab notes
- Completeness of deliverables

---

## Additional Resources

**Official Documentation:**
- Ceph Documentation: https://docs.ceph.com/
- Ceph Architecture: https://docs.ceph.com/en/latest/architecture/

**Community Resources:**
- Ceph Mailing Lists
- Ceph IRC/Slack channels
- GitHub Issues

**Books:**
- "Mastering Ceph" by Nick Fisk
- "Learning Ceph" by Karan Singh

**Practice:**
- Ceph playground environments
- Cloud-based Ceph testing (AWS, Azure with nested virtualization)

---

## Notes for Instructors

**Pre-class Preparation:**
- Ensure lab environment is provisioned and tested
- Prepare backup VMs in case of corruption
- Have Ceph installation files cached locally
- Create troubleshooting cheat sheet

**Time Management:**
- Build in buffer time for troubleshooting
- Some deployments may take longer than expected
- Be prepared to skip advanced topics if behind schedule

**Common Student Issues:**
- Network configuration problems
- Insufficient resources on VMs
- Clock synchronization issues
- Firewall blocking Ceph ports

**Engagement Strategies:**
- Pair programming for complex tasks
- Rotate lab leadership roles
- Encourage experimentation and breaking things
- Share real-world war stories

---

## Post-Course Recommendations

**For Students:**
- Keep lab environment for continued practice
- Join Ceph community forums
- Contribute to Ceph documentation
- Explore cloud-managed Ceph offerings (IBM Cloud, Digital Ocean)

**Advanced Topics for Follow-up:**
- Multi-site replication
- BlueStore internals
- Custom CRUSH maps
- Performance analysis with ceph-perf
- Integration with container orchestrators

**Certifications:**
- Red Hat Certified Specialist in Ceph Storage Administration
- SUSE Certified Administrator in Enterprise Storage

